---
layout: review
title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders"
tags: deep-learning CNN transformer segmentation classification object-detection attention
author: "Pierre Roug√©"
cite:
    authors: "Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie"
    title:   "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders"
    venue: arXiv
pdf: "https://arxiv.org/abs/2301.00808"
---

# Notes

* **Important : See this [review](https://creatis-myriad.github.io./2023/02/23/Conv-Next.html) to understand ConvNeXt V1**
* Also you can refer to this [review](https://creatis-myriad.github.io/2022/08/31/MAE.html) about Masked Autoencoders (MAE)

# Highlights

* ConvNeXt V1 was focus on supervised learning only however, similarly to transformers, convolution based models can benefit from self-supervised learning techniques (such as MAE)
* Simply apply self-supervised learning methods (i.e. MAE) to ConvNeXt leads to sub optimal results
* **This article propose a fully convolutional masked autoencoder framework (MAE) and modify the ConvNeXt architecture with Global Response Normalization (GRN) layers**



# Fully convolutional masked autoencoder

![](/collections/images/convnextV2/framework.jpg) 

* They use a random masking strategy with a masking ratio of 0.6 

* They use ConvNeXt as the encoder

* Unlike transformers you can't simply remove masked patches from the image as the 2D structure of the image must be preserved for convolution

* Also naives solutions such as replace masked patches by *masked tokens* doesn't perform well in practice

* The idea here is to see the masked image as a sparse data, based on that is natural to use sparse convolution that will operate only on the visible pixels
  
  | *Note 1 : if the center pixel of the convolution is masked then the convolution will not operate and just return a masked pixel*

  |  *Note 2 :  the sparse convolution layers can be converted back to standard convolution at the fine-tuning stage without requiring additional handling*
  
* They tested several decoder architecture/depth but at the end they choose a simple ConvNeXt plain decoder 

* Loss function is the mean squared error (MSE) computed only on the masked patches between reconstructed and target images
  
  ## Evaluation of FCMAE

* They pre-train and fine-tune on ImageNet-1K for 800 and 100 epochs respectively

* Ablation study is done to justify design choice

  ![](/collections/images/convnextV2/sparse_conv.jpg)

  ![](/collections/images/convnextV2/ablation1.jpg)

* The also compare the self-supervised approach to fully supervised learning

  <img src="/collections/images/convnextV2/result1.jpg" style="zoom:70%;" />

* **They perform better than the fully supervised setup on 100 epochs but are still worse than the  orginal ConvNeXt V1 baseline train on 300 epochs**

  | *This is in contrast to the recent success of masked image modeling using transformer-based models [..] where the pre-trained models significantly outperform the supervised counterparts.*

# Global Response Normalization

- To try to improve on that and to gain more insight into the learning behavior they perform a qualitative analysis in the feature space.

<img src="/collections/images/convnextV2/feature_collapse.jpg" style="zoom:100%;" />

- They noticed a feature collapse phenomenom and they computed the cosine distance between features to get more insight

  <img src="/collections/images/convnextV2/cosine_distance.jpg" style="zoom:100%;" />

  

- This analysis showed a reduction in feature diversity through the network for the ConvNeXt V1 FCMAE model

- **They propose to introduce a new normalization layer called global response normalization (GRN) to increase feature diversity**

- This layer is composed of three steps:

  - global feature aggregation : consist in mapping each feature map $$X_i$$ into a scalar and construct a vector representing all feature maps

    Here they use the $$L2$$-norm : $$G(X) = \lbrace \Vert X_1 \Vert, \Vert X_2 \Vert, ...,\Vert X_C \Vert \rbrace $$

  - feature normalization :  $$N( \Vert X_i  \Vert) = \frac {\Vert X_i \Vert}{\sum_{j=1,...,C} \Vert X_j \Vert \rbrace}$$

  - feature calibration : $$X_i = X_i * N(G(X)_i)$$

- They add a residual connection to create the final block which is: $$X_i = \gamma * X_i * N(G(X)_i) + \beta + X_i$$, with $$\gamma$$ and $$\beta$$ learnable parameters 

- **They incorporate the GRN layer into the ConvNeXt block creating ConvNeXt V2**

  <img src="/collections/images/convnextV2/convnextv2_block.jpg" style="zoom:100%;" />

## Impact of GRN

* GRN succeed to mitigate the feature collapse behavior (see cosine distance between features maps)
* The new model outperform the 300 epochs supervised counterpart 

<img src="/collections/images/convnextV2/result2.jpg" style="zoom:80%;" />

* Ablation study

  <img src="/collections/images/convnextV2/ablation2.jpg" style="zoom:100%;" />

# ImageNet Experiments 

## Classification

* Comparison with ConvNeXt V1 and contribution of pre-training

<img src="/collections/images/convnextV2/codesign.jpg" style="zoom:100%;" />

* Comparison with SOTA methods

<img src="/collections/images/convnextV2/comparison.jpg" style="zoom:100%;" />

* They also evaluate the performance of the framework when adding an intermediate pretraining on Image-net 22K
* They achieve the best state of the art result on Image-Net 1K using only public dataset

<img src="/collections/images/convnextV2/imagenet-22k.jpg" style="zoom:100%;" />

## Object Detection

<img src="/collections/images/convnextV2/coco.jpg" style="zoom:100%;" />

## Semantic Segmentation

<img src="/collections/images/convnextV2/segmentation.jpg" style="zoom:100%;" />



# Conclusion

The fully convolutional masked autoencoder pre-training allow to improve the performance on various tasks but requires a specific architecture design.
