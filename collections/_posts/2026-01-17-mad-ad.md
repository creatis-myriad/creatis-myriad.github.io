---
layout: review
title: "MAD-AD: Masked Diffusion for Unsupervised Brain Anomaly Detection"
tags: Diffusion models, anomaly detection
author: "Olivier Bernard"
cite:
    authors: "Farzad Beizaee, Gregory Lodygensky, Christian Desrosiers, Jose Dolz"
    title: "MAD-AD: Masked Diffusion for Unsupervised Brain Anomaly Detection"
    venue: "IPMI 2025"
pdf: "https://arxiv.org/pdf/2502.16943"
---

# Notes

* Link to the code [here](https://github.com/farzad-bz/MAD-AD)

# Diffusion model reminders

The full presentation, including links to the diffusion model (DDPM), is available [here](https://olivier-bernard-creatis.github.io//files//research-sfds-bernard-2025.pdf) 

<div style="text-align:center">
<img src="/collections/images/mad-ad/diffusion-model-1.jpg" width=700></div>

<div style="text-align:center">
<img src="/collections/images/mad-ad/diffusion-model-2.jpg" width=700></div>

<div style="text-align:center">
<img src="/collections/images/mad-ad/diffusion-model-3.jpg" width=700></div>

&nbsp;

# Highlights

* Extension the idea proposed in the THOR method to training procedure 
* Learn to remove anomaly regions through the diffusion process
* The same formalism allows the remove of anomally regions with varying size
* Evaluation from three public datasets: `IXI Dataset` (brain MRI scans from
approximately 600 healthy subjects), `ATLAS 2.0` (655 T1-weighted
MRI scans accompanied by expert-segmented lesion masks), and `BraTS'21` (1251 brain scans across four modalities: T1-weighted, contrast-
enhanced T1-weighted - T1CE, T2-weighted, and T2 Fluid Attenuated Inversion
Recovery - FLAIR)

# Motivations

* Remove the needs for the forward / reverse process in the state-of-the-art diffusion models for anomally detection which introduces several weaknesses: 1) the forward diffusion process can cause a loss of distinctive features; 2) The choice of the number of steps introduces a tradeoff between precision of the located anomalies and the size of the anomaly that can be removed.

# Overall idea

* The method is based on the following hypothesis. Starting from a VAE trained on normal subjects only, a region with abnomalities will be efficiently represented by noise in the corresponding latent space. 
* Using a dataset with healthy subjects, it possible to introduce relevant synthetic anomalies by adding varying gaussian noise in random regions through a forward diffusion process and then learn to remove them through a reverse diffusion process. 

# Methodology

## Training procedure

### Modeling the normal feature space

* Excusively healthy subjects are used during training: $$\{x^{(i)}\}_{i=1}^{N}$$ with $$x^{(i)} \in \mathbb{R}^{H \times W \times C}$$ 
* A pre-trained variational auto-encoder $$V_{E,\phi}$$ is fine-tuned on the dataset and then frozen for the rest of the process
* Each input image are $$x^{(i)}$$ are mapped to its latent space representation $$z^{(i)} = V_{E,\phi}\left(x^{(i)}\right)$$, where $$z^{(i)} \in \mathbb{R}^{H' \times W' \times C'}$$

<div style="text-align:center">
<img src="/collections/images/mad-ad/VAE-procedure.jpg" width=700></div>

&nbsp;

### Random masking

* The latent features of an input normal sample $$z_0$$ is spatially divided into non-overlapping patches defined by a random mask $$M \in [0,1]^{H' \times W'}$$
* This random mask simulates region with anormalities
  
### Forward process

* The forward diffusion process gradually applies noise to the masked patches of sample $$z_0$$ for $$t$$ time steps to generate samples $$z_t$$ with $t \in [1, T]$

$$z_t = \left( \sqrt{\bar{\alpha_t}} \, z_0 + \sqrt{1-\bar{\alpha_t}} \, \epsilon \right) \odot M + z_0 \odot \left( 1 - M \right)$$

where $$\epsilon \sim \mathcal{N}(0,I)$$, $$\alpha_t = 1 - \beta_t$$ and $$\bar{\alpha_t} = \prod_{i=1}^{T} \alpha_i$$

<div style="text-align:center">
<img src="/collections/images/mad-ad/forward-process-with-masking.jpg" width=700></div>

&nbsp;

### Reverse process

* The reverse process aims to recover the original data $$z_0$$ by gradually removing the noise  
* Given the sample $$z_t$$ at step $$t$$ and mask $$M$$ at spatial location $$k$$, the reverse process can be modeled as: 

$$p\left(z^k_{t-1} \mid z^k_t\right) = \begin{cases}
\mathcal{N}\left( \mu_{\theta}(z^k_t,t), \, \beta_t \mathbf{I} \right), &\textit{if  } M^k=1 \\
z^k_t, & \textit{otherwise}
\end{cases}$$
  
> $$\mu_{\theta}(z_t,t)$$ is a trainable function, which can be reparameterized as a predicted noise $$\epsilon$$ or a predicted clean image $$z_0$$

* Due to the incorporated random masking strategy, the predicted clean image formulation is chosen:

$$\mu_{\theta}(z_t,t) = \frac{\sqrt{\bar{\alpha}_{t-1}} \, \beta_t}{1-\bar{\alpha}_t} \, f_{\theta,z_0}(z_t,t) + \frac{\sqrt{\alpha_t}\,(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} \, z_t$$

where $$f_{\theta,z_0}(z_t,t)$$ is a function that predicts $$\tilde{z}_0$$ at time $$t$$, given $$z_t$$. 
  
### Mask prediction

<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-generative-model-insight-1.jpg" width=550></div>

&nbsp;

* The latent embedding of the root node $$z_0$$ is sample from a standard Gaussian $$z_0 \sim \mathcal{N}\left( 0, I \right)$$
* The decision of going on the left or the right node is sampled from a Bernoulli distribution $$p(c_0 \mid z_0) = Ber(r_{p,0}(z_0))$$ where $$\{r_{p,i} \mid \, i \in \mathbb{V} \, \backslash \, \mathbb{L} \}$$ are functions parametrized by <span style="color:darkred">neural networks</span> (simple MLP with two hidden layers with 128x2 neurons + leaky ReLU ) defined as <span style="color:blue">routers</span>. $$\mathbb{V}$$ is the set of nodes and $$\mathbb{L}$$ is the set of leaves. 
 
<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-decision-illustration.jpg" width=350></div>

&nbsp; 
  
* The latent embedding of the selected child, e.g. $$z_1$$, is then sampled from a Gaussian distribution $$z_1 \sim \mathcal{N}\left( \mu_{p,1}(z_0), \sigma^2_{p,1}(z_0) \right)$$, where $$\{ \mu_{p,i}, \sigma_{p,i} \, \mid i \in \mathbb{V} \, \backslash \, \{0\} \}$$ are functions parametrized by <span style="color:darkred">neural networks</span> defined as <span style="color:blue">transformations</span>.   

<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-transformation-illustration.jpg" width=350></div>

&nbsp; 

* This process continues until a leaf is reached  
  ➔ $$z_{\mathcal{P}_l} = \{ z_i \mid i \in \mathcal{P}_l \}$$ the set of latent variables selected by the path $$\mathcal{P}_l$$, which goes from the root to the leaf $$l$$  
  ➔ $$pa(i)$$ the parent node of the node $$i$$  
  ➔ $$p(c_{pa(i) \to i} \mid z_{pa(i)})$$ the probability of going from $$pa(i)$$ to $$i$$  
  ➔ $$\mathcal{P}_l$$ defines the sequence of decisions  
&nbsp; 
* The prior probability of the latent embeddings and the path given $$\mathcal{T}$$ can be summarized as:  
  ➔ <span style="color:darkred">$$p_{\theta}(z_{\mathcal{P}_l}, \mathcal{P}_l) = p(z_0) \, \prod_{i\in \mathcal{P}_l \backslash \{0\}}{\, \, \underbrace{p(c_{pa(i) \to i} \mid z_{pa(i)})}_{\text{path}} \, \cdot \, \underbrace{p(z_i \mid z_{pa(i)})}_{\text{latent variable}}}$$</span>   
  
* Finally, $$x$$ is sampled from a distribution that is conditionned on the selected leaf $$p_{\theta}(x \mid z_{\mathcal{P}_l}, \mathcal{P}_l) = \mathcal{N}(\mu_{x,l}(z_l), \sigma_{x,l}^2(z_l))$$, where $$\{ \mu_{x,l} , \sigma_{x,l} \mid l \in \mathbb{L}\}$$ are functions parametrized by <span style="color:darkred">leaf-specific neural networks</span> defined as <span style="color:blue">decoders</span>  
  
<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-decoder.jpg" width=350></div>

&nbsp;
  
## Inference model 
  
<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-inference-model-insight-1.jpg" width=700></div>

&nbsp;
  
* Described by the variational posterior distribution of both the latent embeddings $$z_{\mathcal{P}_l}$$ and the paths $$\mathcal{P}_l$$
* The probability of the root and of the decisions are now conditioned on the sample $$x$$  
  ➔ <span style="color:darkred">$$q(z_{\mathcal{P}_l}, \mathcal{P}_l \mid x) = q(z_0 \mid x) \, \prod_{i\in \mathcal{P}_l \backslash \{0\}}{\, \, \underbrace{q(c_{pa(i) \to i} \mid x)}_{\text{path}} \, \cdot \, \underbrace{q(z_i \mid z_{pa(i)})}_{\text{latent variable}}}$$</span>   
  
* The authors used the work of [Sonderby et al.](https://arxiv.org/abs/1602.02282) to compute the variational probability distribution of the latent embeddings $$q(z_0 \mid x)$$ and $$q(z_i \mid z_{pa(i)})$$  
  ➔ $$q(z_0 \mid x) = \mathcal{N}( \mu_{q,0}(x), \sigma^2_{q,0}(x) )$$  
  ➔ $$q(z_i \mid z_{pa(i)}) = \mathcal{N}( \mu_{q,i}(z_{pa(i)}), \sigma^2_{q,i}(z_{pa(i)}) ) \,$$, $$\, \forall i \in \mathcal{P}_l$$  
&nbsp;
* First, a deterministic a bottom-up pass computes the node-specific approximate contributions:  
  ➔ $$d_h = \text{MLP}(d_{h+1})$$  
  ➔ $$\hat{\mu}_{q,i} = \text{Linear}(d_{depth(i)}) \,$$, $$\, i \in \mathbb{V}$$  
  ➔ $$\hat{\sigma}^2_{q,i} = \text{Softplus}(\text{Linear}(d_{depth(i)})) \,$$, $$\, i \in \mathbb{V}$$  
  ➔ where $$d_H$$ is parametrized by a <span style="color:darkred">domain-specific neural network</span> defined as <span style="color:blue">encoder</span>  
  
<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-encoder.jpg" width=250></div>

&nbsp;
  
  ➔ $$\text{MLP}(d_h)$$ for $$h \in \{1,\cdots,H\}$$ are <span style="color:darkred">neural networks</span> shared among the parameter predictors, $$\hat{\mu}_{q,i}, \hat{\sigma}^2_{q,i}$$ at the same depth  
  ➔ they are characterized by the same architecture as the transformations used in the generative model  

* A stochastic downward pass then recursively computes the approximate posteriors  
  ➔ $$\sigma^2_{q,i} = \frac{1}{ \hat{\sigma}^{-2}_{q,i} \, + \, \sigma^{-2}_{p,i} }$$  
  ➔ $$\mu_{q,i} = \frac{ \hat{\mu}_{q,i} \, \cdot \, \hat{\sigma}^{-2}_{q,i} \,+\, \mu_{p,i} \, \cdot \, \sigma^{-2}_{p,i} }{ \hat{\sigma}^{-2}_{q,i} \, + \, \sigma^{-2}_{p,i} }$$  

<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-posteriors.jpg" width=350></div>

&nbsp;
  
* Finally, the variational distributions of the decisions $$q(c_i \mid x)$$ are defined as    
  ➔ $$q(c_i \mid x) = q(c_i \mid d_{\text{depth(i)}}) = Ber(r_{q,i}(d_{\text{depth(i)}}))$$  
  ➔ where $$\{ r_{q,i} \, \mid \, i \in \mathbb{V} \, \backslash \, \mathbb{L} \}$$ are functions parametrized by <span style="color:darkred">neural networks</span> and are characterized by the same architecture as the routers of the generative model
  
## Learning process

* The parameters of both the generative model (defined as $$p$$) and inference model (defined as $$q$$), consisting of the encoder $$(\mu_{q,0}, \sigma_{q,0})$$, the transformations ($$\{ (\mu_{p,i},\sigma_{p,i}), (\mu_{q,i},\sigma_{q,i}) \, \mid \, i \in \mathbb{V} \backslash \{0\} \}$$), the decoders ($$\{ \mu_{x,l}, \sigma_{x,l} \, \mid \, l \in \mathbb{L} \}$$) and the routers ($$\{ r_{p,i}, r_{q,i} \, \mid \, i \in \mathbb{V} \, \backslash \, \mathbb{L} \}$$) are learned by maximizing the ELBO

* Each leaf $$l$$ is associated with only one path $$\mathcal{P}_l$$. The data likelihood conditioned on $$\mathcal{T}$$ can be written as:  
  ➔ $$p(x \mid \mathcal{T}) = \sum_{l \in \mathbb{L}}{\int_{z_{\mathcal{P}_l}}}{p(x,z_{\mathcal{P}_l},\mathcal{P}_l)} \, \,$$ (use of the of the marginal)  
  ➔ $$p(x \mid \mathcal{T}) = \sum_{l \in \mathbb{L}}{\int_{z_{\mathcal{P}_l}}}{p_{\theta}(z_{\mathcal{P}_l}, \mathcal{P}_l) \, \cdot\,  p_{\theta}(x \mid z_{\mathcal{P}_l},\mathcal{P}_l)} \, \,$$ (use of the of Bayes' formula)  
    
* Use variational inference to derive the ELBO of the log-likelihood  
  ➔ $$\mathcal{L}(x \mid \mathcal{T}) := \underbrace{\mathbb{E}_{q(z_{\mathcal{P}_l},\mathcal{P}_l \mid x)}[ \text{log} \, p(x \mid z_{\mathcal{P}_l},\mathcal{P}_l) ]}_{\text{data fidelity}} \, - \, \underbrace{\text{KL}(q(z_{\mathcal{P}_l},\mathcal{P}_l \mid x) \parallel p(z_{\mathcal{P}_l},\mathcal{P}_l) )}_{\text{distributions fit}}$$  

* See paper for all the derivations ;)  
  ➔ <span style="color:darkred">Use of Monte Carlo framework to estimate some distributions !</span>  

## Data fidelity term
* The data fidelity term can be expressed as:  
  ➔ $$\mathcal{L}_{rec} = \mathbb{E}_{q(z_{\mathcal{P}_l},\mathcal{P}_l \mid x)}[ \text{log} \, p(x \mid z_{\mathcal{P}_l},\mathcal{P}_l) ]$$ <br>
  ➔ <span style="color:darkred">$$\mathcal{L}_{rec} \approx \frac{1}{M} \sum_{m=1}^{M} \sum_{l\in \mathbb{L}}{P(l;c)\log (\mathcal{N(\mu_{x,l}(z_l^{(m)}),\sigma_{x,l}^2(z_l^{(m)})))}}$$</span> <br><br>   
  - with $$P(i;c)=\prod_{j\in P_{i \backslash \{0\}}}q(c_{pa(j) \rightarrow j} \mid x)$$ for $$i \in \mathbb{V}$$ the probability of reaching node $$i$$, which corresponds to the product over the probabilities of the decisions in the path until $$i$$
  - with $$z_l^{(m)}$$ the Monte Carlo (MC) samples, and $$M$$ the number of MC samples.
  
## Distributions fit term
* The distributions fit term can be expressed as:  
  ➔ <span style="color:darkred">$$\text{KL}(q(z_{\mathcal{P}_l},\mathcal{P}_l \mid x) \parallel p(z_{\mathcal{P}_l},\mathcal{P}_l) ) = \text{KL}_{root} + \text{KL}_{nodes} + \text{KL}_{decisions}$$</span><br>
  ➔ $$\text{KL}_{root} = \text{KL}(q(z_0 \mid x) \parallel p(z_0))$$ <br>
  ➔ $$\text{KL}_{nodes} \approx \frac{1}{M} \sum_{m=1}^{M} \sum_{i \in \mathbb{V} \backslash \{0\}} P(i;c) \, \text{KL}(q(z_i^{(m)} \mid pa(z_i^{(m)})) \parallel p(z_i^{(m)} \mid pa(z_i^{(m)})))$$ <br>
  ➔ $$\text{KL}_{decisions} \approx \frac{1}{M} \sum_{m=1}^{M} \sum_{i \in \mathbb{V} \backslash \{\mathbb{L}\}} P(i;c) \, \text{KL}(q(c_i \mid x) \parallel p(c_i \mid z_i))$$ <br>



# Experiments

* Evaluation on 8 public datasets (small images): MNIST, Fashion-MNIST, 20Newsgroups, Omniglot, Omniglot-5, CIFAR-10 and CIFAR-100, CelebA
* Comparison with baseline methods: VAE (non-hierarchical method) and LadderVAE (sequential method)
* The dimension of all latent embeddings $$z = \{z_0, \cdots, z_V \}$$ is the same and is equal to 8 for MNIST, Fashion, and Omniglot, to 4 for 20Newsgroups, and to 64 for CIFAR-10, CIFAR-100, and CelebA
* The maximum depth of the tree is set to 6 for all datasets, except 20Newsgroups where depth was increased to 7 to capture more clusters
* To compute DP and LP, the tree is allowed to grow to a maximum of 30 leaves for 20Newsgroups and CIFAR-100, and 20 for the rest, while for ACC and NMI the number of leaves is set to the number of true classes
* The transformations consist of one-layer MLPs of size 128 and the routers of two-layers of size 128 for all datasets except for the real-world imaging data where the size of the MLP is increased to 512
* the encoder and decoders consist of simple CNNs and MLPs
* The trees are trained for $$N_t = 150$$ epochs at each growth step, and the final tree is finetuned for $$N_f = 200$$ epochs
* All experiments were run on RTX3080 GPUs
* Training TreeVAE with 10 leaves on MNIST, Fashion-MNIST, and Omniglot-50 takes between 1h and 2h, Omniglot-5 30 minutes, CIFAR-10 5h
* Training TreeVAE with 20 leaves on 20Newsgroup takes approximately 30 minutes, and on CIFAR-100 9h
* Training TreeVAE on CelebA takes approx 8h

&nbsp;

# Results

## Clustering performances

* Assessement of the hierarchical clustering performance by computing dendrogram purity (DP) and leaf purity (LP), , as defined by [Kobren et al.](https://nmonath.github.io/talks/perch_data_science_symposium17.pdf), and the more standard clustering metrics: accuracy (ACC) and normalized mutual information (NMI), by setting the number of leaves for TreeVAE and for the baselines to the true number of clusters

<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-result-clustering-performance.jpg" width=700></div>

&nbsp;

## Generative capacities

* Compute the approximated true log-likelihood (LL) calculated using 1000 importance-weighted samples, together with the ELBO and the reconstruction loss (RL)

<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-result-generative-capacity.jpg" width=700></div>

&nbsp;

## Discovery of Hierarchies

* In addition to solely clustering data, TreeVAE is able to discover meaningful hierarchical relations between the clusters, thus allowing for more insights into the dataset

<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-result-figure4.jpg" width=700></div>

&nbsp;

<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-result-figure6.jpg" width=500></div>

&nbsp;

<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-result-figure7.jpg" width=700></div>

&nbsp;


# Conclusions

* This paper presents an unsupervised clustering-based VAE method
* The model architecture / design is strongly inspired by decision trees
* Results vary with key parameters (max number of leafs, depth, different thresholds) that need to be manually selected
* The method seems computationally expensive
