---
layout: review
title: "Tree Variational Autoencoders"
tags: varitional encoders, clustering
author: "Olivier Bernard"
cite:
    authors: "Laura Manduchi, Moritz Vandenhirtz, Alain Ryser, Julia Vogt"
    title: "Tree Variational Autoencoders"
    venue: "NeurIPS 2023"
pdf: "https://arxiv.org/abs/2306.08984"
---

# Notes

* Link to the code [here](https://github.com/lauramanduchi/treevae)

# VAE reminders

The full presentation, including links to the diffusion model (DDPM), is available [here](https://olivier-bernard-creatis.github.io//files//research-sfds-bernard-2025.pdf) 

<div style="text-align:center">
<img src="/collections/images/tree-vae/vae-1.jpg" width=700></div>

<div style="text-align:center">
<img src="/collections/images/tree-vae/vae-2.jpg" width=700></div>

<div style="text-align:center">
<img src="/collections/images/tree-vae/vae-3.jpg" width=700></div>

<div style="text-align:center">
<img src="/collections/images/tree-vae/vae-4.jpg" width=700></div>

<div style="text-align:center">
<img src="/collections/images/tree-vae/vae-5.jpg" width=700></div>

<div style="text-align:center">
<img src="/collections/images/tree-vae/vae-6.jpg" width=700></div>

<div style="text-align:center">
<img src="/collections/images/tree-vae/vae-7.jpg" width=700></div>

&nbsp;

# Highlights

* Develop a  deep unsupervised probabilistic approach for hierarchical clustering 
* Extension of VAE framework to tree-based posterior distribution over latent variables 
* Complex distrubutions are approwimated through Monte Carlo techniques
* Integration of a contrastive loss to reinforce accurate and contextually meaningfull clustering
* Evaluation from several public datasets: MNIST, Fashion-MNIST, 20Newsgroups and Omniglot-5

# Motivations

* Unsupervised clustering

# Methodology

## Overall architecture

* Analogy with the classical VAE

<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-vs-vae.jpg" width=700></div>

&nbsp;

* ***Generation of the binary tree $$\mathcal{T}$$***  

<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-tree-generation.jpg" width=500></div>

&nbsp;

  ➔ Predefine the maximum depth (in this example 4)  
  ➔ Training a tree composed of a root and two leaves for $$N_t$$ epochs by optimizing the ELBO  
  ➔ Once the model converged, a leaf is selected and two children are attached to it. The leaf criteria can vary according to the application. In these experiments, the authors chose to select the nodes with the number of samples higher than a threshold to retain balanced leaves  
  ➔ The sub-tree composed of the new leaves and the parent node is then trained for $$N_t$$ epochs by freezing the weights of the rest of the model and by optimizing the ELBO. At this stage, the sub-tree is trained using only the subset of data that have a high probability (higher than a threshold) of being assigned to the parent node  
  ➔ The process is repeated until the tree reaches its maximum depth or until a condition (e.g. predefined maximum number of leaves) is met  
  ➔ The entire model is then fine-tuned for $$N_f$$ epochs by unfreezing all weights. During this stage, the tree is pruned by removing almost empty branches (with the expected number of assigned samples lower than a threshold)  
  

* ***Modeling of the generative model***  

<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-generative-model-insight-1.jpg" width=700></div>

&nbsp;

  ➔ The latent embedding of the root node $$z_0$$ is sample from a standard Gaussian $$z_0 \sim \mathcal{N}\left( 0, I \right)$$  
  ➔ The decision of going on the left or the right node is sampled from a Bernoulli distribution $$p(c_0 | z_0) = Ber(r_{p,0}(z_0))$$ where $$\{r_{p,i} | \, i \in \mathbb{V} \, \backslash \, \mathbb{L} \}$$ are functions parametrized by neural networks (simple MLP with two hidden layers with 128x2 neurons + leaky ReLU ) defined as <span style="color:blue">routers</span>. $$\mathbb{V}$$ is the set of nodes and $$\mathbb{L}$$ is the set of leaves. 
 
<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-decision-illustration.jpg" width=350></div>

&nbsp; 
  
  ➔ The latent embedding of the selected child, e.g. $$z_1$$, is then sampled from a Gaussian distribution $$z_1 \sim \mathcal{N}\left( \mu_{p,1}(z_0), \sigma^2_{p,1}(z_0) \right)$$, where $$\{ \mu_{p,i}, \sigma_{p,i} \, \mid i \in \mathbb{V} \, \backslash \, \{0\} \}$$ are functions parametrized by neural networks defined as <span style="color:blue">transformations</span>.   

<div style="text-align:center">
<img src="/collections/images/tree-vae/tree-vae-transformation-illustration.jpg" width=350></div>

&nbsp; 

  ➔ This process continues until a leaf is reached  
  ➔ $$z_{\mathcal{P}_l} = \{ z_i \mid i \in \mathcal{P}_l \}$$ the set of latent variables selected by the path $$\mathcal{P}_l$$, which goes from the root to the leaf $$l$$  
  ➔ $$pa(i)$$ the parent node of the node $$i$$  
  ➔ $$p(c_{pa(i) \to i} \mid z_{pa(i)})$$ the probability of going from $$pa(i)$$ to $$i$$  
  ➔ $$\mathcal{P}_l$$ defines the sequence of decisions  
  ➔ The prior probability of the latent embeddings and the path given $$\mathcal{T}$$ can be summarized as $$p_{\theta}(z_{\mathcal{P}_l}, \mathcal{P}_l) = p(z_0) \, \prod_{i\in \mathcal{P}_l \backslash \{0\}}{\,p(c_{pa(i) \to i} \mid z_{pa(i)}) \, p(z_i \mid z_{pa(i)})}$$   
  ➔ Finally, x is sampled from a distribution that is conditionned on the selected leaf $$p_{\theta}(x \mid z_{\mathcal{P}_l}, \mathcal{P}_l) = \mathcal{N}(\mu_{x,l}(z_l), \sigma_{x,l}^2(z_l))$$, where $$\{ \mu_{x,l} , \sigma_{x,l} \mid l \in \mathbb{L}\}$$ are functions parametrized by leaf-specific neural networks defined as <span style="color:blue">decoders</span>  
  
  
  
  
* ***Finetuning stage***  
  ➔ Train a video LDM (VLDM) to generate controllable video  
  ➔ Extend the 2D UNet to the 3D counterpart using a simple inflation strategy  
  ➔ Inflate all the spatial convolution layers at the temporal dimension with t=1  
  ➔ Insert a temporal self-attention layer following the cross-attention layer in each block [^1]  
  ➔ Two types of conditioning: local conditions for fine-grained control (<span style="color:blue">sketch</span>, <span style="color:blue">time-series masks</span>, <span style="color:blue">time-series mitral valve skeleton</span>, <span style="color:blue">time-series optical flow</span>) and global conditions for coarse-grained control (<span style="color:blue">image prior</span> encoded from a pretrained MedSAM image encoder and <span style="color:blue">text</span> from the pretrained CLIP text encoder)  
  ➔ Two conditioning scheme: summation + concatenation from the input noisy image for the local conditions and cross-attention for global conditions (just like in the pretraining stage)  
  ➔ Global condition injection method: in this case $$Q^T, Q^I \in \mathbb{R}^{B F \times H \times W \times C}$$, $$K^T, K^I, V^T, V^I \in \mathbb{R}^{B F \times N \times C}$$, with $$F$$ the length of frames
  
<div style="text-align:center">
<img src="/collections/images/heart-beat/heartbeat-second-stage.jpg" width=550></div>

&nbsp;

# Experiments

* Validation on the CAMUS dataset: 884 echocardiographic videos: 431 apical two-chamber (A2C) and 453 apical four-chamber (A4C)
* The dataset was split randomly into 793 and 91 videos with 16 frames for training and testing at the patient level
* Text prompts (i.e., "An ECHO with 2/4-chamber view.") were set for all videos according to the actual view
* For few-shot generalization validation, the authors employed 50 CMR volumes from M&Ms Challenge as the training set
* All frames / slices were resized to $$256 \times 256$$
* LDM was developed upon Stable Diffusion and initialized using the public pretrained weights
* During finetuning, the spatial weights were frozen except for the newly added optical flow encoder and the temporal layers which were also kept trainable
* Batch sizes of 64 and 16 for the first and second stages of training

> During the whole training, all conditions were jointly used. This way, the model did not have to
be finetuned for each unique combination of multimodal conditions every time, enabling the flexibility to drop several conditions during inference

* The CMR synthesizer was merely conditioned on mask volumes, and initialized with US-trained HeartBeat.
* The learning rate was initialized to 1e-4 after 500 steps of warm-up strategy and decayed by a cosine annealing scheduler
* All models were trained using the Adam optimizer for 200 epochs
* The models of the last epoch were selected.
* 4 NVIDIA A6000 GPUs were used during training

&nbsp;

# Results

## Qualitative results

* Several controlled guided sequences were generated to visually assess to impact of the different conditions:  
  ➔ Image prior-controlled echocardiography video synthesis  
  ➔ Sketch-controlled echocardiography video synthesis  
  ➔ Mitral valve motion-controlled echocardiography video synthesis  
  ➔ Various conditions-controlled echocardiography video synthesis  
  ➔ Generalization to 3D CMR synthesis

<div style="text-align:center">
<img src="/collections/images/heart-beat/heart-beat-results-part1-1.jpg" width=600></div>

&nbsp;

<div style="text-align:center">
<img src="/collections/images/heart-beat/heart-beat-results-part1-2.jpg" width=600></div>

&nbsp;

<div style="text-align:center">
<img src="/collections/images/heart-beat/heart-beat-results-part2-1.jpg" width=600></div>

&nbsp;

<div style="text-align:center">
<img src="/collections/images/heart-beat/heart-beat-results-part2-2.jpg" width=600></div>

&nbsp;


## Quantitative results

* Comparison with two other existing methods: VideoComposer and MoonShot
* Three metrics were used to evaluate the performance  
  ➔ Fréchet Inception Distance (FID) for image quality evaluation at the feature level  
  ➔ Fréchet Video Distance (FVD) for visual quality and temporal consistency assessment at the video level  
  ➔ Structure Similarity Index (SSIM) score to assess the controllability  

<div style="text-align:center">
<img src="/collections/images/heart-beat/heart-beat-results-table.jpg" width=700></div>

&nbsp;

# Conclusions

* This paper presents a novel diffusion-based HeartBeat framework for controllable and flexible echocardiography video synthesis
* Multimodal control information were integrated, including local and global conditions to separately provide fine- and coarse-grained guidance
* Generalization was investigated from echocardiography 2D+t to 3D cardiac MRI image synthesis 
* The interest of the integrated text condition ("An ECHO with 4/2-chamber view") appears to be limited but opens the door for more sophistigated conditionings 

&nbsp;

# References

[^1]: Video diffusion models, J. Ho et al., 2022

