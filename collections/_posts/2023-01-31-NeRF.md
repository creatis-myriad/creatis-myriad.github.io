---
layout: review
title: "NeRF Representing Scenes as Neural Radiance Fields for View Synthesis"
tags: deep-learning scene-representation view-synthesis
author: "Maylis Jouvencel"
cite:
    authors: "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
    title:   "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"
    venue:   "ECCV 2020"
pdf: "https://arxiv.org/pdf/2003.08934.pdf"
---

# Notes
* Link to code, full paper and more examples in the [project page](https://www.matthewtancik.com/nerf)

# Highlights
* The objective of the paper is to synthesize novel views of complex scenes
* The authors use a MLP to optimize a continuous volumetric scene function on a sparse set of input views

# Method
## Neural Radiance Fields
Each scene is represented by a MLP network $ F_\Theta : (\bold{x},\bold{d}) \rightarrow (RGB,\sigma)$ 


see figure ...

archtecture is as such ...

## Classical volume rendering

Rendering density $$\sigma(x)$$ = differential probability of a ray terminating at a infinitesimal particle Â at location x (= proba that the ray is stopped by an object at that location) => expected color at a certain camera ray is computed using the density at points queried with stratified approach

## Optimizing NeRF

The quality of the resconstructed scenes is not that great...
see figure ...

Two methods are used to avoid this:

### Positional encoding
### Hierarchical sampling




# Results

# Conclusion

 
 

 The network architecture:
 ![](/article/images/MyReview/UNetArchitecture.png)

 A U-Net is based on Fully Convolutional Networks (FCNNs)[^1].

 The loss used is a cross-entropy:
 $$ E = \sum_{x \in \Omega} w(\bold{x}) \log (p_{l(\bold{x})}(\bold{x})) $$

 # References

 [^1]: Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional
       networks for semantic segmentation (2014). arXiv:1411.4038.