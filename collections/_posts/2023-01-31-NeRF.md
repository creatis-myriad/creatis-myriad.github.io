---
layout: review
title: "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"
tags: deep-learning scene-representation view-synthesis
author: "Maylis Jouvencel"
cite:
    authors: "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
    title:   "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"
    venue:   "ECCV 2020"
pdf: "https://arxiv.org/pdf/2003.08934.pdf"
---

# Notes
* Link to code, full paper and more examples in the [project page](https://www.matthewtancik.com/nerf)

# Highlights
* The objective of the paper is to synthesize novel views of complex scenes
* The authors use a MLP to optimize a continuous volumetric scene function on a set of 2D input views.

# Method

![](/collections/images/NeRF/pipeline.jpg)

*Figure 1: pipeline proposed by the authors*

For one input camera view, the authors sample a set of coordinates along the camera ray and feed it to the network, which produces the color and density at these locations.

New output views are rendered and gradient descent is used to optimize the model.

## Neural Radiance Fields
Each scene is represented by an 8 fully-connected layers MLP network $$ F_\Theta : (\boldsymbol{x},\boldsymbol{d}) \rightarrow (RGB,\sigma)$$ with $$x$$ being the postion, $$d$$ the direction and $$\sigma$$ the density of a sampled point.


## Volume rendering

Classical image rendering techniques are used to accumulate colors and densities in order to synthesize novel 2D views of the scene.


## Optimizing NeRF

#### Positional encoding

The resconstructed scenes can be blurry in areas with complex details.

![](/collections/images/NeRF/positional_encoding.png)

*Figure 2: Visualization of the effect of positional encoding*

The inputs are mapped to a higher dimensional space by using high frequency functions before being fed to the network:
$$ \gamma(p) = (\sin(2^{0}\pi p),\cos(2^{0}\pi p), ... ,\sin(2^{L-1}\pi p),\cos(2^{L-1}\pi p)  ) $$.

This enables the learning of higher frequencies variations in the object.

#### Hierarchical sampling
Instead of sampling regularly along a camera ray, and in order to avoid uselessly sampling a lot of free space, two networks are trained.

A first "coarse" network is trained on a widely spaced set of points. This gives information about the density in certain area of the the space. Then the "fine" network is trained on points sampled on areas that are expected to contain more visible content.

This enables a more efficient sampling of the space.

## Implementation

The loss used is : 
$$ \mathcal{L} = \sum_{r\in \mathcal{R}} [ \|\hat{C}_c(\boldsymbol{r}) - C(\boldsymbol{r}) \|_{2}^{2} + \|\hat{C}_f(\boldsymbol{r}) - C(\boldsymbol{r}) \|_{2}^{2}] $$

with $$\mathcal{R}$$ the set of rays in each batch, $$C(\boldsymbol{r})$$, $$\hat{C}_c(\boldsymbol{r})$$ and $$\hat{C}_f(\boldsymbol{r})$$ the RGB colors for ground truth, coarse and fine volume predicted.

Note that the density does not appear in this loss, it is used to compte de RGB colors.

Optimizing a network usually takes around 12 hours.

# Results

This method can be used on real word or synthetic scenes. A comparison is done with three networks based on deep learning techniques

![](/collections/images/NeRF/results_synthetic.png)

*Figure 3: results on a synthetic dataset*

![](/collections/images/NeRF/results_realworld.png)

*Figure 4: results on a real-world dataset*

Quatitatively, the results also achives more satisfying results than state-of-the-art, using metrics that evaluate the output 2D views.

![](/collections/images/NeRF/results_table.png)

*Table 1: quantitative comparaison with state-of-the-art method [metrics: SNR/SSIM (higher is better), LPIPS (lower is better))*


Another advatage of this method is that, once trained, the network is lighter than the set of input images used for training. This makes it useful to overcome the issue of storage problem implied by the use voxelized representation of complex scenes.

# Conclusion

- Scenes can be represented by a MLP taking position and view angle as input and outputting color and density: this is the nature of Neural Radiance Fields intrduced by this paper
- The renderings are better with the method proposed than with state-of-the-art
