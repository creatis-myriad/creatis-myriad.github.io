---
layout: review
title: "DETR : End-to-End Object Detection with Transformers"
tags: object-detection transformer deep-learning
author: "Baptiste Pierrard, Robin Trombetta"
cite:
    authors: "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko"
    title:   "End-to-End Object Detection with Transformers"
    venue: "European Conference on Computer Vision (ECCV) 2020"
pdf: "https://arxiv.org/pdf/2005.12872.pdf"
---

# Highlights

* DEtection TRansformer (DETR) is a new transformer-based model to perform object detection with an end-to-end fashion.
* The main idea of the main is to force unique prediction via bipartite matching, avoiding components like non-maximum suppressor or anchors.
* It combines a CNN-based feature extraction with a transformer encoder-decoder architecture
* Code is available on this [GitHub repo](https://github.com/facebookresearch/detr)

&nbsp;

# Introduction

Recently, the Segment Anything Model has had a resounding impact on the field of foundation models for image segmentation. Its main strength is to leverage a huge dataset with 1.1B masks in 11M images annotated in a three-stage process. Even though it has shown amazing performances at zero-shot transfer learning on many tasks, these performances poorly generalize to medical images[^2]. As SAM has only been trained on natural images, that are semantically far far from medical images, this result may not be that surprising. Moreover, it has been shown that SAM's abilities vary a lot depending on the dataset, the task and the input prompt (Fig. 1). 

# Model architecture

<div style="text-align:center">
<img src="/collections/images/detr/architecture.jpg" width=800></div>
<p style="text-align: center;font-style:italic">Figure 1. High-level overview of DETR architecture.</p>

DETR model is composed of 4 main parts :
* a CNN backbone feature extractor
* a transformer encoder that acts of the features extracted by the backbone CNN
* a transformer decoder that performs cross-attention between object queries and visual features output by the image encoder
* a prediction head that converts the outputs of the decoder to position and class bouding boxes predictions

Figure 1 gives a high-level overview of the architecture of DETR and Figure 2 showh it with more precisely. The details of DETR's architecture are discussed in the following subsections.

## Feature extractor

Given an input image $$x \in \mathbb{R}^{3 \times H_0 \times W_0}$$, a ImageNet-pretrained ResNet-50 or 101 is used to extract visual features $$f \in \mathbb{R}^{2048 \times \frac{H_0}{32} \times \frac{W_0}{32}}$$. Note that this module could be replaced by a classical patch extractor (usually parametrized by a unique strided convolution) but extracting features from a model pretrained on natural images yield better results.

## Transformer Encoder

The image encoder is a stack of transformer encoder blocks, where multi-head self-attention is performed between image tokens. It has a slight modification from the original architecture as a fixed sinusoidal positional encoding is added at *each* block. A 1x1 convolution precedes the encoder to reduce the input feature size from $$C$$ to $$d$$.

## Transformer Decoder

The inputs of the decoder are $$N$$ embeddings of size $$d$$. $$N$$ represents the maximum number of possible object predicted in an image, and each embdding represents a future bounding box predicted by the network (after the decoder and the feed-forward prediction head).

To be more precise, the input embedding are *learnt* positional encodings, in this context also called **object queries**.

Contrary to the original architecture, there is no masking in the decoder, *i.e.* the model is not auto-regressive. 

## Prediction feed-forward networks

The final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d. A firstlinear layer outputs 4 scalars to represent the normlized center coordinated, height and width of the box, and a second linear layer predicts the class label associated with the box using a softmax function.
It is important to note that each embedding output by the decoder is decoded *independently* from the other.
Since there is in general less than $$N$$ objects present in the image, an additional class $$\emptyset$$ ("no object") is added.


<div style="text-align:center">
<img src="/collections/images/detr/detailled_architecture.jpg" width=400></div>
<p style="text-align: center;font-style:italic">Figure 2. Details of the encoder-decoder architecture of DETR.</p>

# Object detection set prediction loss



# References
[^1]: A. Kirillov, E. Mintun, N. Ravi,H. Mao, C. Rolland,L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg,  W.-Y. Lo, P. Dollar, R. Girshick, *Segment Anything*, ICCV 2023, [link to paper](https://arxiv.org/pdf/2304.02643.pdf)
