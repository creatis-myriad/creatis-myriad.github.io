---
layout: review
title: "Masked Autoencoders Are Scalable Vision Learners"
tags: deep-learning attention transformer self-supervised
author: "Gaspard Dussert"
cite:
    authors: "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, Ross Girshick"
    title:   "Masked Autoencoders Are Scalable Vision Learners"
    venue:   "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 16000-16009"
pdf: "https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf"
---

# Highlight

* Masked Auto-Encoder is a very simple self-supervised training approach giving state-of-the-art results
* Asymmetric encoder-decoder architecture that reduce the computation cost
* Does not rely too much on data augmentation : only crops are used

# Method

![](/collections/images/mae/architecture.jpg)

#### Masking

The image is split into patches as in the original ViT, then 75% of the patches are masked (removed) following a uniform distribution

#### Encoder

It uses the ViT architecture and only take the tokens of visible patchs

#### Decoder

* It also uses the ViT architecture, but is smaller than the encoder
* Masked token are added to the sequence before being fed to the decoder
* The final layer of the decoder is a linear layer that predicts pixel values for each token
* The masked token are shared and learn during the training

#### Loss

* The loss is the MSE between the groundtruth pixel values and the predicted pixel values
* The loss is only computed on the masked tokens !
* However, the authors report that the accuracy drop only slightly if the loss is computed on all tokens

#### Experiment

* Dataset : ImageNet-1K
* Model :
	 * Encoder : ViT-Large (24 blocks, width of 1024)
	 * Decoder : 8 blocks with a width of 512
* Training time :  16 hours for 800 epochs, using 128 TPU-v3 cores. (but it is faster than the other methods)

# Results 

#### Masking ratio

They find an optimal masking ratio of 75%	

![](/collections/images/mae/acc_mask_ratio.jpg)

#### Examples from the validation set

![](/collections/images/mae/examples.jpg)

#### Increasing the masking ratio

Note : the masking ratio was increased at inference time, the model was still trained with a masking ratio of 75%

![](/collections/images/mae/example_mask_ratio.jpg)

#### Accuracy 

First they do self-supervied pre-training and then a finetuning,   the top-1 validation accuracy is reported below :

$$
\begin{array}{ccc}
\text { scratch, original } & \text { scratch, our impl. } & \text { baseline MAE } \\
\hline 76.5\% & 82.5\% & 84.9\%
\end{array}
$$

#### Comparison with the state of the art

![](/collections/images/mae/mae_vs_sota.jpg)