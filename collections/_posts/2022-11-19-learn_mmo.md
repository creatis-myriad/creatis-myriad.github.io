---
layout: review
title: "Learning Maximally monotone operators for image recovery"
author: "Sophie Carneiro Esteves"
cite:
    authors: "Jean-Christophe Pesquet, Audrey Repetti, Matthieu Terris, and Yves Wiaux"
    title:   "Learning Maximally monotone operators for image recovery"
    venue:   "SIAM Journal on Imaging Sciences"
pdf: "https://arxiv.org/pdf/2012.13247.pdf"
---

# Notes
- Code is available on GitHub: [https://github.com/basp-group/PnP-MMO-imaging](https://github.com/basp-group/PnP-MMO-imaging)


# Inverse problem and notations
Many applications can be defined as a minimisation of two energy terms : 

$$\operatorname*{minimize}_{x \in \mathcal{H}} f(x) + g(x)$$

with : 
- $$f$$ the data fidelity term
- $$g$$ the regularization term


Let $$\Gamma(\mathcal{H})$$ be the set of lower semi-continuous convex functions from $$\mathcal{H}$$ to $$]-\infty, +\infty]$$. Let $$f$$ and $$g$$ belong to  $$\Gamma(\mathcal{H})$$ and $$\partial f$$ and $$\partial g$$ their Moreau's subdifferential. $$x$$ is a solution if :

$$0 \in \partial f(x) + \partial g(x)$$

Under these assumptions, we can show that it is a special case of the following monotone inclusion problem:

$$ \operatorname{Find} x \in \mathcal{H} \operatorname{such that} 0 \in \partial f(x) + A(x) $$

with:
- $$A$$ a maximally monotone operator (MMO) 

To solve this problem, many methods has been introduced such as the forward backward algorithm which is expressed as : 

$$ x_{n+1} = J_{\gamma A}(x_n - \gamma \Delta f(x_n))$$

with: 
- $$ J_{\gamma A} = (Id + A)^{-1} $$ the resolvant of the MMO $$A$$

- $$\begin{align} & \gamma > 0 \end{align}$$ a constant


# Introduction
Recently, plug-and-play approaches has been proposed to solve a regularized variational problem by replacing the operator related to the regularization term with a more sophisticated denoiser: a neural network. However, the convergence of the algorithm is originally ensured under some conditions on $$f$$ and $$g$$ (see above). We do not know the mathematical properties of the neural network, therefore we do not have any guarantee of the convergence of the plug-and-play approach.

Objectif du papier : garantir la convergence du plug and play

en apprenant un NN possédant des proprietes mathématiques garantissant la convergence de l'approche: MMO

 They proposed to learn a Maximally Monotone in a supervised manner to replace the resolvant with a neural network that check conditions to assure the convergency of the variational optimisation scheme.


# Highlights

In this article, they propose: 
- A theorem that prove that they can approximate the resolvant of a Maximally Monotone Operator (MMO) with a neural network
- A framework to be able to learn the resolvant of a MMO.

# Method

The idea is to learn is the resolvant of the maximally monotone operator $$A$$ : $$J_{\gamma A}$$. After training, the learned resolvant can be injected in the iterative method to solve a regularized variational problem such as the forward backward algorithm : 

$$ x_{n+1} = \tilde{J}_{\theta}(x_n - \gamma \Delta f(x_n))$$

with: 
-  $$\tilde{J}_{\theta}$$ the learned resolvant of the MMO $$A$$ with $$\theta$$ parameters

#### Learning a maximally monotone operator


##### Architecture of the neural network
To learn a maximally monotone operator, the neural network needs to be nonexpensive therefore the nonlinear activation functions needs to be nonexpensive that means 1-Lipschitzian. Fortunately most of usual activation functions are 1-Lipschitzian.

However this is not enough to be sure that a MMO is learned. Therefore, some constraints are added during the training through the definition of the loss in order to force the learning of an MMO. 

##### Loss 

One of the property of a MMO is the following:

> A is a MMO if and only if there exists a nonexpansive (i.e. 1-Lipschitzian) operator $$Q$$ such that $$J_A(x) = \frac{x + Q(x)}{2}$$ that is $$A = 2(Id + Q)^{-1} -Id$$

Thus to learn a MMO, they proposed to force $$Q$$ to be nonexpensive with the following constraint: 

$$ \left\|\nabla Q_\theta(x)\right\| \leqslant 1 $$

With:
- $$\nabla Q$$ the jacobian of the operator $$Q$$.


Therefore they propose to define the following loss : 

$$
\underset{\theta}{\operatorname{minimize}} \sum_{\ell=1}^L \left\|\tilde{J}_\theta\left(y_{\ell}\right)-\bar{x}_{\ell}\right\|^2+\lambda \max \left\{\left\|\nabla Q_\theta\left(\tilde{x}_{\ell}\right)\right\|^2, 1-\varepsilon\right\}
$$

with 
- $$\bar{x}_l$$ an image and $$ y_l $$ its noisy observation 
- $$\varepsilon$$ a constant to ensure that $$ \left\|\nabla Q_\theta(x)\right\| \leqslant 1 - \varepsilon$$
- $$\nabla \tilde{Q}_\theta(x) = \nabla 2 \tilde{J}_\theta - Id$$
- $$\tilde{x}_{\ell}=\varrho_{\ell} \bar{x}_{\ell}+\left(1-\varrho_{\ell}\right) \tilde{J}_\theta\left(y_{\ell}\right)$$, $$\varrho$$ an random variable with uniform distribution on $$[0, 1]$$

#### Ensure the convergence

##### Universal approximation theorem
Let $$A$$ be a stationnary MMO. 
For every compact set $$ S \subset \mathcal{H}$$ and for every $$\varepsilon \in ]0, + \infty[ $$ there exists a nonexpensive neural network $$ Q_\varepsilon$$ such as  $$A_\varepsilon = 2(Id + Q_\varepsilon)^{-1} -Id$$ that satisfies: 

- $$\forall x \in S, \left\| J_A - J_{A_{\varepsilon}} (x)\right\| \leqslant \varepsilon$$
- Let $$x \in \mathcal{H}$$ and $$y \in A(x)$$ such as $$x + y \in S$$. Than there exx



##### Convergence


# Results

![]()


# References

